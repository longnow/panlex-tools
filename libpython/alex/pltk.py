#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# PanLex ToolKit

import time

import unicodedata
import regex as re
from unidecode import unidecode
from bs4 import Tag
from time import sleep

def preprocess(entries):
    # perform across-the-board preprocessing of things that should !ALWAYS! be done
    result = []
    for entry in entries:
        processed_entry = []
        for col in entry:

            # nonstandard spaces/newlines/control characters
            col = re.sub(r'[\x19\u001f\xa0\x7f\u200BÂ—\u200E\uFEFF\n]', ' ', col).strip()
            
            # PUA
            # col = re.sub(r'îŒ²îî°îªîŒî', '', col).strip()

            # fullwidth punctuation, numbers
            col = col.replace('ï¼Ÿ', '?')
            col = col.replace('ï¼', '!')
            # if re.search(r'\p{Nd}', col):
            #     col = unicodedata.normalize('NFKC', col).strip()

            # parenthesis standardization
            if '(' in col or ')' in col:
                col = col.replace('ï¼ˆ','(').replace('ï¼‰',')')

            # hyphen to hyphen-minus
            col = col.replace('â€','-')

            # "etc"
            col = re.sub(r'[\,ã€;]?\s*(etc| u\.Ã¤\.?)\.?$', '', col).strip()
            # ellipses
            col = re.sub(r'ãƒ»ãƒ»ãƒ»', ' â€¦ ', col)
            col = re.sub(r'__+', ' â€¦ ', col)
            col = re.sub(r'[ï½ã€œ]', ' â€¦ ', col)
            col = re.sub(r'\.\s*\.(\s*\.)+', ' â€¦ ', col).strip()
            col = re.sub(r'\s*â€¦\s*', ' â€¦ ', col).strip()
            col = re.sub(r'^\s*â€¦', '', col).strip()
            col = re.sub(r'â€¦\s*$', '', col).strip()

            # excess whitespace
            col = re.sub(r'  +', ' ',col).strip()
            col = re.sub(r'\( +','(',col).strip()
            col = re.sub(r' +\)',')',col).strip()

            # weirdly placed commas
            col = re.sub(r'\s* ,([^\s])', r', \1', col).strip()

            # digit separator commas
            col = re.sub(r'(\d),(\d\d\d)', r'\1\2', col).strip()
            col = re.sub(r'(\d)\.(\d\d\d)', r'\1\2', col).strip()

            # surprise html encoded chars
            col = col.replace('&amp;', '&')
            col = col.replace('&quot;', '"')

            processed_entry.append(col)
        result.append(processed_entry)

    return result


PARENS = [(r'\(',r'\)'),(r'\[',r'\]'),(r'\{',r'\}'),(r'ï¼ˆ',r'ï¼‰'),(r'ã€',r'ã€‘')]

def split_outside_parens(entries, cols, delim=r',', parens=PARENS, max_wd_single_ex=999):
    ''' Peforms a split of each specified column, but ignores anything in parens.
    entries   = list of entries, which are lists of columns
    cols      = list of columns (element indices) on which to perform operation
    delim     = regex of delimiter(s) at which to split
    parens    = list of tuples of opening and closing characters (regex escaped)
                to be considered parenthetical
    max_wd_single_ex = maximum number of words in a single section indicating
                       split should not be performed (definitional) '''

    # PLEASE REWRITE THIS SOON WITH A REGEX LOOKBEHIND INSTEAD OF THIS WEIRD HACK
    
    SOP_DELIM = 'ïœœ'
    TEMP_PAREN = [(r'ğŸ¾',r'ğŸ‚Š')]
    
    # detect sentences
    parens += TEMP_PAREN
    minwords = 4

    assert parens

    # o_parens = [p[0] for p in parens]
    # c_parens = [p[1] for p in parens]

    not_in_parens_regex = r'(?<!{}){}'.format(r'|'.join(r'{}[^{}]*'.format(o, c) for o, c in parens), delim)
    
    result = []

    # paren_re = make_paren_regex(parens) + r'+\s*'

    for entry in entries:

        if not isinstance(entry, list):
            raise ValueError(entry, 'not a list; did you remember to split tabs yet?')

        for col in cols:

            try: entry[col]
            except: raise ValueError('index', col, 'not in entry:', entry)

            if not ''.join(entry[col]).startswith('â«·dfâ«¸'):

                # detect sentences/other non splittable things and put special parens around them
                # sentences: start w/ capital letter, end with period(s)
                entry[col] = re.sub(r'(\p{Lu}(?:'+DEFAULT_PR_NOCAP+r'|[^\s\(\)\[\].])+(?:\s+(?:'+DEFAULT_PR_NOCAP+r'|[^\s\(\)\[\].])+){'+str(minwords)+r',}[\.!?]+)', TEMP_PAREN[0][0]+r'\1'+TEMP_PAREN[0][1], entry[col])
                # commas separating decimals or digit groups
                entry[col] = re.sub(r'(\d+(?:,\d+)+)', TEMP_PAREN[0][0]+r'\1'+TEMP_PAREN[0][1], entry[col])

                # count = 0
                # 
                # entry_letters = []
                # 
                # for l in entry[col]:
                #     if list(filter(None, [re.match(o_paren, l) for o_paren in o_parens])):
                #         # if letter is open paren
                #         entry_letters.append(l)
                #         count += 1
                #     elif list(filter(None, [re.match(c_paren, l) for c_paren in c_parens])):
                #         # if letter is close paren
                #         entry_letters.append(l)
                #         count -= 1
                #     elif count == 0 and re.match(delim, l):
                #         entry_letters.append(SOP_DELIM)
                #     else:
                #         entry_letters.append(l)
                
                entry_split = re.split(not_in_parens_regex, entry[col])

                # test for long single expressions; if so, ignore split entirely
                if len(entry_split) > 1 and max(len(s.strip().split(' ')) for s in entry_split) >= max_wd_single_ex:
                    # print([len(s.strip().split(' ')) for s in entry_split])
                    # print(entry_split)
                    entry_split = [entry[col]]
                

                # entry[col] = [re.sub(r'['+TEMP_PAREN[0][0]+TEMP_PAREN[0][1]+r']', '',    c).strip() for c in ''.join(entry_letters).split(SOP_DELIM)]
                entry[col] = [re.sub(r'['+TEMP_PAREN[0][0]+TEMP_PAREN[0][1]+r']', '', c).strip() for c in entry_split]

            else:
                entry[col] = [entry[col]]

        result.append(entry)
    return result



def make_paren_regex(parens=PARENS, maxnested=10, cap=True):
    ''' Makes a regex to match any parenthetical content, up to a certain number
            of layers deep.
    parens    = list of tuples of opening and closing characters (regex escaped)
                to be considered parenthetical
    maxnested = max number of nested parens to match '''

    paren_res = []
    for p in parens:
        o, c = p
        oc = o + c
        fld  = (o + r'(?:[^' + oc + r']|') * (maxnested - 1)
        fld += o + r'[^' + c + r']*' + c
        fld += (r')*' + c) * (maxnested - 1)
        paren_res.append(fld)
    result = r'|'.join(paren_res) + r')'
    result = r'(' + result if cap else r'(?:' + result
    return result

def remove_parens(s, parens=PARENS):
    return re.sub(DEFAULT_PR, '', s).strip()


DEFAULT_PR_NOCAP = make_paren_regex(cap=False)
DEFAULT_PR = make_paren_regex()

EXDFPREP_RULES = {
    'eng-000' : {
        1: {
            r'^(be|become|stay|remain) or (be|become|stay|remain) (.*)$' : (r'\1 \3â€£\1 \2', '', ''),
            r'(^| )big bird($)' : (r'\1(big) bird\2', 'â«·mcs2:art-300â«¸IsAâ«·mcs:eng-000â«¸bird', ''),
            r'^you (sg|pl)\.?$' : (r'you (\1)', '', ''),
            r'(^| )potatoe( |$)' : (r'\1potato\2', '', ''),
            r'(^| )lightening( |$)' : (r'\1lightning\2', '', ''),
            r'(^| )to[ \-]day( |$)' : (r'\1today\2', '', ''),
            r'^the last one$' : (r'(the last one)', '', ''),
        },
        2: {
            r'([^\s])\s+(s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)(?:(?: or |\s*/\s*)s(?:\-|o(?:me|em))(?: other )?(?:one|body|thing))?|s\.[bot]\.?|o\.s\.?)([^\'â€™]|$)' : (r'\1 (\2)\3', '', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)(?:(?: or |\s*/\s*)s(?:\-|o(?:me|em))(?: other )?(?:one|body|thing))?|s\.[bot]\.?|o\.s\.?)\s+([^\s])' : (r'\1(\2) \3', '', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)[\'â€™]?s)\s+([^\s])' : (r'\1(\2) \3', '', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:(?:for (?:something|s\.t\.?))?\(?\s*to\s*\)?\s+)?be)\s+([^\(])'    : (r'\1(\2) \3', '', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal'),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(\(?(?:a\s+)?(?:small |large |big )?(?:kind|variety|type|sort|species) of(?: an?(?= ))?\)?|k\.?o\.)\s*' : (r'\1(\2) ', r'', ''),
        },
        3: {
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)([Tt]he|[Aa]n?)\s+((?:'+DEFAULT_PR_NOCAP+r'\s*)*\s*[^\(\)\[\]\s]+)$'     : (r'\1(\2) \3', '', ''),            # r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(the)\s+([^\(])'     : (r'\1(\2) \3', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)\((s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)(?:(?: or |\s*/\s*)s(?:\-|o(?:me|em))(?: other )?(?:one|body|thing))?|s\.[bot]\.?|o\.s\.?)\)\s+(which|that|who|to)' : (r'\1\2 \3', '', ''),
            r'\((s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)(?:(?: or |\s*/\s*)s(?:\-|o(?:me|em))(?: other )?(?:one|body|thing))?|s\.[bot]\.?|o\.s\.?)\)\s+(else(?:\'s)?)' : (r'(\1 \2)', '', ''),
            r'^\((s(?:\-|o(?:me|em))?(?: other )?(?:body|one|thing)(?:(?: or |\s*/\s*)s(?:\-|o(?:me|em))(?: other )?(?:one|body|thing))?|s\.[bot]\.?|o\.s\.?)\)\s+' : (r'\1 ', '', ''),
            r'^((?:[^\s\(\)\[\]]+\s)?)((?:'+DEFAULT_PR_NOCAP+r')?\s*)(\(?(?:kind|variety|type|sort|species) of(?: an?)?\)?|\(?k\.?o\.\)?)\s*([^\s]+ ?[^\s]+)$' : (r'\2 (\3) \1\4', r'â«·mcs2:art-300â«¸IsAâ«·mcs:eng-000â«¸\4', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r')?\s*)(\(?(?:a\s+)?(?:kind|variety|type|sort|species) of(?: an?)?\)?|\(?k\.?o\.\)?)\s*([^\s]+ ?[^\s]+)$' : (r'\1 (\2) \3', r'â«·mcs2:art-300â«¸IsAâ«·mcs:eng-000â«¸\3', ''),
        },
        4: {
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:not )?)[Tt]o\s+(?!(?:'+DEFAULT_PR_NOCAP+r'$|the|you|us|him|her|them|me|[wt]?here|no|one[\'â€™]s|oneself|a[n])(?: |$))' : (r'\1\2(to) ', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
            r'(^| )make to ' : (r'\1make (to) ', '', ''),
        },
        5: {
            r'(^|\s)\(a\) (lot|bit|posteriori|priori|fortiori|few|little|minute|same|while|propos|cappella)(\s|$)' : (r'\1a \2\3', r'', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:\(to\) )?)(become)\s+([^\s\()][^\s]*)$' : (r'\1\2\3 \4', r'â«·mcs2:art-316â«¸Inchoative_ofâ«·mcsâ«¸\4', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:\(to\) )?)(make\s+)((?:\(to\)\s+)?)((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(?!(?:[ei]nquiries|arrangements|preparations|confession|tributary|progress|oneself|profits|amends|again|entry|haste|merry|signs|also|even|sure|use|art|for|it|up|space|room|fence|out|love)(?: |$))([^\s\()][^\s]*)((?:\s*'+DEFAULT_PR_NOCAP+r')*)$'     : (r'\1\2\3\4\5\6\7', r'â«·mcs2:art-316â«¸Causative_ofâ«·mcsâ«¸\6', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:\(to\) )?)(cause to\s+)([^\s\()][^\s]*)$'     : (r'\1\2\3\4', r'â«·mcs2:art-316â«¸Causative_ofâ«·mcsâ«¸\4', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:\(to\) )?)(stay|remain)\s+([^\s\()][^\s]*)$'     : (r'\1\2 (\3) \4', 'â«·mcs2:art-303â«¸AspectPropertyâ«·mcs:art-303â«¸DurativeAspect', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal'),
        },
        6: {
            # r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)\(([Tt]he|[Aa]n?)\)\s+((?:(?:'+DEFAULT_PR[1:-1]+'|[^\(\)\[\]\s]+))(?: (?:'+DEFAULT_PR[1:-1]+'|[^\(\)\[\]\s]+))?)$'     : (r'\1(\2) \3', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
            r'^do it$' : ('â«·dfâ«¸do it', '', ''),
            r'fÃ¦ces' : ('faeces', '', ''),
            r'^to become$' : ('(to) become', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
            r'\(the\) whole lotâ«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun' : ('the whole lot', '', ''),
        },
    },
    'jpn-000' : {
        1: {
            r'^(.+)(ã§ã‚ã‚‹)$' : (r'\1(\2)', '', ''), # to be ~
            r'([\p{Han}\p{Katakana}]'+DEFAULT_PR_NOCAP+r'?)(ã |ã®|ãª)$' : (r'\1(\2)', '', ''),
            r'^(.*[\p{Han}])(ã‚‰ã›ã‚‹)$' : (r'\1\2', r'â«·mcs2:art-316â«¸Causative_ofâ«·mcsâ«¸\1ã‚‹', ''),
            r'^(ã•ã›ã‚‹)$' : (r'\1', r'â«·mcs2:art-316â«¸Causative_ofâ«·mcsâ«¸ã™ã‚‹', ''),
            r'^(ãŒ)(\p{Han})' : (r'(\1)\2', r'', ''),
            # r'^(ã‚’)(\p{Han})' : (r'(\1)\2', r'', ''),
            r'^(ã‚’)' : (r'(\1)', r'', ''),
            r'(ãªã©)$' : (r'(\1)', '', ''),    # ... etc.
            r'^([^â€¦]+)(ããªã‚‹)$' : (r'\1\2', r'â«·mcs2:art-316â«¸Inchoative_ofâ«·mcsâ«¸\1ã„', ''),    # to become ~ (keiyoshi, ''),
            r'^([^â€¦]+)(ã«ãªã‚‹)$' : (r'\1\2', r'â«·mcs2:art-316â«¸Inchoative_ofâ«·mcsâ«¸\1', ''),    # to become ~
            r'^[â€¦\s]*(ã«)(ãªã‚‹)$' : (r'(\1)\2', '', ''),    # to become
        },
        2: {
            r'å…¶('+DEFAULT_PR_NOCAP+r'?)\(ã®\)' : (r'å…¶\1ã®', '', ''),
        },
    },
    'arb-000' : {
        1: {
            r'^(Ø§Ù„)' : (r'(\1)', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''), # definite article
        }
    },
    'spa-000' : {
        1 : {
            #r'([^\s])\s+\(?(alg(?:ui[eÃ©]n|o|\.))\)?([^\'â€™])' : (r'\1 (\2)\3', '', ''),
            #r'^\(?(alg(?:ui[eÃ©]n|o|\.))\)?\s+([^\s\(])' : (r'(\1) \2', '', ''),
            r'^\(?([Ee]l|[Uu]n)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''), # Masculine not determined
            r'^\(?([Ll]os|[Uu]nos)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r'^\(?([Ll]a|[Uu]na)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
            r'^\(?([Ll]as|[Uu]nas)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r'^\(?([Ss]er|[Ee]star)\)?\s+([^\s\(])'    : (r'(\1) \2', '', ''),
            #r'\s+of(\s*(?:\([^\)]*\)\s*)*)$'                    : (r' (of)\1', '', ''),
            r'^\s*Â¡|!\s*$'    : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Interjection', ''),
            r'[Â¿\?]' : ('', 'â«·mcs2:art-303â«¸ForcePropertyâ«·mcs:art-303â«¸InterrogativeForce', ''),
            r'[Â¡\!]' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Interjection', ''),
            r'^idioma ([^\s\(][^\s]*)$' : (r'(idioma) \1', '', ''),
            r'^(\(?(?:una?\s+)?(?:clase) de\)?)\s*([^\s]+ ?[^\s]+)$' : (r'(\1) \2', r'â«·mcs2:art-300â«¸IsAâ«·mcs:spa-000â«¸\2', ''),
        },
        2 : {
            r' \(m\.?\)$' : ('', 'â«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            r' \(f\.?\)$' : ('', 'â«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
        }
    },
    'cat-000' : {
        1 : {
            r'[Â¿\?]' : ('', 'â«·mcs2:art-303â«¸ForcePropertyâ«·mcs:art-303â«¸InterrogativeForce', ''),
        },
    },
    'nld-000' : {
        1 : {
            r'^soort (\S*)$' : (r'(soort) \1', r'â«·mcs2:art-300â«¸IsAâ«·mcs:nld-000â«¸\1', ''),
        },
        2 : {
            r'^soort (.*)$' : (r'(soort) \1', r'', ''),
        },
    },
    'ile-000' : {
        1 : {
            r'^\(?([Ll][ie])\)?\s+([^\s\(])'    : (r'(\1) \2',    'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
            r'^\(?([Uu]n)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
        },
    },
    'ast-000' : {
        1 : {
            r'^\(?([Ee]l)\)?\s+([^\s\(])'    : (r'(\1) \2',    'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            r'^\(?([Ll]os)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r'^\(?([Ll]a)\)?\s+([^\s\(])'    : (r'(\1) \2',    'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
            r'^\(?([Ll]es)\)?\s+([^\s\(])'    : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
        }
    },
    'gle-000' : {
        1 : {
            r'^(an)\s+([^\(])' : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''), # def art
        }
    },
    'als-000' : {
        1 : {
            r'^(i/e|e/i|tÃ«|[ie])\s+([^\(])' : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Adjectival', ''),
        },
    },
    'nob-000' : {
        1 : {
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)((?:\(?\s*Ã¥\s*\)?\s+)?bli)\s+([^\(])'    : (r'\1(\2) \3', '', ''),
        },
        2: {
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)Ã¥\s+' : (r'\1(Ã¥) ', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
        },
    },
    'dan-000' : {
        1: {
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(e[nt])\s+' : (r'\1(\2) ', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
        },
    },
    'fra-000' : {
        1 : {
            r'^(.+)\s+(quelqu[\'â€™]un|quelque chose)' : (r'\1 (\2)', '', ''),
            r'\(?(q\.?q\.?(?:ch|un?|n)\.?)\)?' : (r'(\1)', '', ''),
            r'^([Ll][\'â€™])([^\s\(])' : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
            r'^([Ll]e)\s+([^\s\(])'         : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            r'^([Ll]a)\s+([^\s\(])'         : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
            r'^([Ll]es)\s+([^\s\(])'        : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r'^([Uu]n)\s+([^\s\(])'         : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            r'^([Uu]ne)\s+([^\s\(])'        : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
            r'^(Ãª\.|Ãªtre)\s+([^\s\(])'     : (r'(\1) \2', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Adjectival', ''),
            r'^(\(?(?:type|sorte|espÃ¨ce) d[e\']\)?)\s*(.+)$' : (r'(\1) \2', r'â«·mcs2:art-300â«¸IsAâ«·mcs:fra-000â«¸\2', ''),
        },
        2: {
            r'^devenir\s+([^\s]+)((?:\s*'+DEFAULT_PR_NOCAP+r')?$)' : (r'devenir \1\2', r'â«·dcs2:art-316â«¸Inchoative_ofâ«·dcsâ«¸\1', ''),
            r' \(m\.?\)$' : ('', 'â«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            r' \(f\.?\)$' : ('', 'â«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
        }
    },
    'isl-000' : {
        1 : {
            r'Ä‘' : (r'Ã°', '', ''),
            r'Ä' : (r'Ã', '', ''),
        },
        2 : {
            r'^((?:[Aa]Ã° )?vera)\s+'     : (r'(\1) ', '', ''),
            r'^(?:[Aa]Ã°)\s+([^\s]+)$'     : (r'(aÃ°) \1', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
            r'^(?:[Aa]Ã°)\s+([^\s]+)'     : (r'(aÃ°) \1', '', ''),
            r'^(verÃ°a)\s+([^\s]+)$'     : (r'\1 \2', r'â«·dcs2:art-316â«¸Inchoative_ofâ«·dcsâ«¸\2', ''),
            r'([^\s]\s)((?:e\-(?:[Ã°imnstu]|ar?|rs?)|ei(?:nhver(?:[ns]|j(?:ar?|ir|um?)|r(?:i|ar?))?|tthv(?:aÃ°|ert)))(?: sem er(?![^ ]))?)(,?\s(?!sem )|ã€|$)' : (r'\1(\2)\3', '', ''),
            r'(^|\s)((?:e\-(?:[Ã°imnstu]|ar?|rs?)|ei(?:nhver(?:[ns]|j(?:ar?|ir|um?)|r(?:i|ar?))?|tthv(?:aÃ°|ert)))(?: sem er(?![^ ]))?)(,?\s(?!sem )[^\s,]|ã€)' : (r'\1(\2)\3', '', ''),

            # e-a    einhverja, einhverra     (fem acc sg, msc acc pl; msc/fem/neu gen pl, ''),
            # e-ar   einhverjar, einhverrar   (fem nom/acc pl; fem gen sg, ''),
            # e-Ã°    eitthvaÃ°      (neu nom/acc sg substant, ''),
            # e-i    einhverri     (fem dat sg, ''),
            # e-m    einhverjum    (msc dat sg, msc/fem/neu dat pl, ''),
            # e-n    einhvern      (msc acc sg, ''),
            # e-r    einhver       (msc/fem nom sg, ''),
            # e-(r)s einhvers      (msc/neu gen sg, ''),
            # e-t    eitthvert     (neu nom/acc sg demonstr, ''),
            # e-u    einhverju     (neu dat sg, ''),

            # r'^([^z]*)z([^z]*)$'     : (r'\1z\2', r'â«·ex:isl-000â«¸\1s\2'), # 1973 reforms
        },
        3 : {
            # r'â«·ex:isl-000â«¸([^zâ«·]*)z([^zâ«·]*)'     : ('', '', ''),
            r'^(?:[Aa]Ã°)\s+([^\s]+)$'     : (r'(aÃ°) \1', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
            r'\) eÃ°a \('     : (r' eÃ°a ', '', ''),

        },
    },
    'ces-000' : {
        # a (takÃ©) = and (also, ''),
        1 : {
            r'^(bÃ½t)\s+'     : (r'(\1) ', '', ''), # to be
            r'^(nÄ›jakÃ½)\s+' : (r'(\1) ', '', ''), # some, a(n, ''),
        },
        2: {
        }
    },
    'fin-000' : {
        1 : {
            r'^(olla)\s+'     : (r'(\1) ', '', ''),
        },
        2: {
            r'^(tulla)\s+(.+)$' : (r'\1 \2', r'â«·dcs2:art-316â«¸Inchoative_ofâ«·dcsâ«¸\2', ''),
        }
    },
    'deu-000' : {
        1 : {
            r'((?:etw(?:\.|as)|jemand(?:en)?)(?: oder etw(?:\.|as))?(?:,? (?:der|was))?)([^\'â€™]|$)' : (r'(\1)\2', '', ''),
            r'\s+(d\.[ih]\..*)$' : (r' (\1)', '', ''),
            r'(^| )(o\.Ã¤\.)( |$)' : (r'\1(\2)\3', '', ''),
            r'^((?:'+DEFAULT_PR_NOCAP+r'\s*)?)(ein(?:e[mnrs]?)?|die|das|de[mnrs])\s+([^\(])'     : (r'\1(\2) \3', '', ''),
        },
        2: {
            r' *\(m\.?(\s*sg\.?)?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸SingularNumber', ''),
            r' *\(f\.?(\s*sg\.?)?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸SingularNumber', ''),
            r' *\(n\.?(\s*sg\.?)?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸NeuterGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸SingularNumber', ''),
            r' *\(m\.?\s*pl\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r' *\(f\.?\s*pl\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r' *\(n\.?\s*pl\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸NeuterGenderâ«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
        },
    },
    'rus-000' : {
        1: {
            r'(^|[^ ] +)((?:Ñ‡Ñ‚Ğ¾|Ñ‡ĞµĞ³Ğ¾|Ñ‡ĞµĞ¼Ñƒ|Ñ‡[ĞµÑ‘]Ğ¼|Ñ‡ÑŒĞ¸Ñ…|Ñ‡ĞµĞ¹|ĞºÑ‚Ğ¾|ĞºĞ¾Ğ³Ğ¾|ĞºĞµĞ¼|ĞºĞ¾Ğ¼Ñƒ?)\-Ğ»(?:Ğ¸Ğ±Ğ¾|\.)?(?: Ğ² (?:Ñ‡Ñ‚Ğ¾|Ñ‡ĞµĞ³Ğ¾|Ñ‡ĞµĞ¼Ñƒ|Ñ‡[ĞµÑ‘]Ğ¼|Ñ‡ÑŒĞ¸Ñ…|Ñ‡ĞµĞ¹|ĞºÑ‚Ğ¾|ĞºĞ¾Ğ³Ğ¾|ĞºĞµĞ¼|ĞºĞ¾Ğ¼Ñƒ?)\-Ğ»(?:Ğ¸Ğ±Ğ¾|\.)?)?)( |$)' : (r'\1(\2)\3', '', ''),
            r' (Ğ¸ Ñ‚\.\s*Ğ¿\.)' : (r' (\1)', '', ''),
        
            # something:    Ñ‡Ñ‚Ğ¾-Ğ» (n/a), Ñ‡ĞµĞ³Ğ¾ (g), Ñ‡ĞµĞ¼Ñƒ (d), Ñ‡ĞµĞ¼ (i), Ñ‡Ñ‘Ğ¼ (p, ''),
            # someone:        ĞºÑ‚Ğ¾-Ğ» (n), ĞºĞ¾Ğ³Ğ¾ (g/a), ĞºĞ¾Ğ¼Ñƒ (d), ĞºĞµĞ¼ (i), ĞºĞ¾Ğ¼ (p, ''),
        },
        2: {
            r'^\(((?:Ñ‡Ñ‚Ğ¾|Ñ‡[ĞµÑ‘]Ğ¼|Ñ‡ĞµĞ¼Ñƒ|Ñ‡ĞµĞ³Ğ¾|Ñ‡ÑŒĞ¸Ñ…|ĞºĞ¾Ğ³Ğ¾|ĞºĞµĞ¼|ĞºĞ¾Ğ¼Ñƒ)\-Ğ»\.?(?: Ğ² (?:Ñ‡Ñ‚Ğ¾|Ñ‡[ĞµÑ‘]Ğ¼|Ñ‡ĞµĞ¼Ñƒ|Ñ‡ĞµĞ³Ğ¾|ĞºĞ¾Ğ³Ğ¾|ĞºĞµĞ¼|ĞºĞ¾Ğ¼Ñƒ)\-Ğ»\.?)?)\)$' : (r'\1', '', ''),
        }
    },
    'general' : {
        998 : {
            r'(^\s*â€¦|â€¦\s*(?:$|â«·))' : ('', '', ''),
            # # r'^((?:[^\s][^\s]?\.)+)$' : (r'\1.', '', ''),
            r'\s+(\.(?:$|â«·))' : (r'\1', '', ''),
            r'\s*(\))\s*\.\s*($|â«·)' : (r'\1\2', '', ''),
        },
        999 : {
            # r' ?\(n\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Noun', ''),
            r' ?\(adj\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Adjectival', ''),
            r' ?\(v\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Verbal', ''),
            r' ?\(v\.?i\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸IntransitiveVerb', ''),
            r' ?\(v\.?t\.?\)$' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸TransitiveVerb', ''),
            r' ?\(sg\.?\)$' : ('', 'â«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸SingularNumber', ''),
            r' ?\(sing(?:\.|ular)\)$' : ('', 'â«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸SingularNumber', ''),
            r' ?\([pP]l?\.?\)$' : ('', 'â«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r' ?\(pl(?:\.|ural)\)$' : ('', 'â«·dcs2:art-303â«¸NumberPropertyâ«·dcs:art-303â«¸PluralNumber', ''),
            r' ?\(excl?(\.|usive)?\)$' : ('', 'â«·dcs2:art-303â«¸PersonPropertyâ«·dcs:art-303â«¸FirstPersonExclusive', ''),
            r' ?\(incl?(\.|usive)?\)$' : ('', 'â«·dcs2:art-303â«¸PersonPropertyâ«·dcs:art-303â«¸FirstPersonInclusive', ''),
            r' ?\(impol(\.|ite)?\)$' : ('', 'â«·dcs2:art-317â«¸registerâ«·dcs:art-317â«¸vulgarRegister', ''),
            r' ?\(pol(\.|ite)?\)$' : ('', 'â«·dcs2:art-317â«¸registerâ«·dcs:art-306â«¸POL', ''),
            # r'\(n?m\.?\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸MasculineGender', ''),
            # r'\(n?f\.?\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Nounâ«·dcs2:art-303â«¸GenderPropertyâ«·dcs:art-303â«¸FeminineGender', ''),
            # r'\(a(?:dj|g)\.?\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Adjectival', ''),
            # r'\(art\.\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Article', ''),
            # r'\(ad?v\.?\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Adverbal', ''),
            # r'\(excl\.?\)' : ('', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-306â«¸EXCLAM', ''),
            # r'\(fig\.\)' : ('', 'â«·dcs2:art-303â«¸SemanticPropertyâ«·dcs:art-297â«¸3.5.2.3', ''),
            # r'\(pop\.\)' : ('', 'â«·dcs:art-317â«¸registerâ«·dcs:art-317â«¸colloquialStyle', ''),
            # r'\(volg\.?\)' : ('', 'â«·dcs:art-317â«¸registerâ«·dcs:art-317â«¸vulgarRegister', ''),
            # r'\(vulg\.?\)' : ('', 'â«·dcs:art-317â«¸registerâ«·dcs:art-317â«¸vulgarRegister', ''),
            r'(.)[ï¼!]$'    : (r'\1', 'â«·dcs2:art-303â«¸PartOfSpeechPropertyâ«·dcs:art-303â«¸Interjection', ''),
            r'(.)[ï¼Ÿ\?]((?:\s*'+DEFAULT_PR_NOCAP+r')*)$' : (r'\1\2', 'â«·mcs2:art-303â«¸ForcePropertyâ«·mcs:art-303â«¸InterrogativeForce', ''),
            r'^\?+$' : ('', '', ''),
            r'^[\-â€”]+$' : ('', '', ''),
            r'[â€"â€œâ€]'    : ('', '', ''),
            # delete periods at end, but only when no periods in exp already, and len > 6
            r'^([^\.]{6,})\s*\.$' : (r'\1', '', ''),
            r'^\(([^\(\)ï¼ˆï¼‰]*)\)$' : (r'â«·dfâ«¸\1', '', ''),
            r'^\[([^\[\]ï¼½ï¼»]*)\]$' : (r'â«·dfâ«¸\1', '', ''),
            r'^ï¼ˆ([^\(\)ï¼ˆï¼‰]*)ï¼‰$' : (r'â«·dfâ«¸\1', '', ''),
            # numbered/lettered entries in front
            r'^[ĞĞ‘Ğ’Ğ“Ğ”Ğ•Ğ–Ğ—Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·ABCDEFGHabcdefgh\d]\)\s+' : (r'', '', ''),
        }
    }
}


def exdfprep(entries, sourcecols, tocol=-1, lang='eng-000', pretag_special_lvs=True, othercol=-1):
    ''' Parenthesizes "definitional" parts of an expression, and adds appropriate pretagged elements.
    entries    = list of entries, lists of elements; column with expressions to be process must be a list
    sourcecols = list of columns (element indices) on which to perform operation
    tocol      = new col in which to deposit pretagged content (end); if -1, use end of source col
    othercol   = new col in which to deposit "other language" content; if -1 discard '''
    
    result = []

    paren_re = DEFAULT_PR
    
    for entry in entries:

        if tocol >= 0: # prepare new column, update source col indices
            entry.insert(tocol, '')
            sourcecols = [col + 1 if col >= tocol else col for col in sourcecols]
            
        if othercol >= 0: # prepare new column, update source col indices
            entry.insert(othercol, '')
            sourcecols = [col + 1 if col >= othercol else col for col in sourcecols]

        if len(lang) == 3: lang += '-000'

        if lang not in EXDFPREP_RULES:
            print('WARNING: Language not supported by exdfprep:', lang)
            print('Reverting to general rules')
            lang = 'general'

        for col in sourcecols:

            try: entry[col]
            except: raise ValueError('index', col, 'not in entry:', entry)

            if not ''.join(entry[col]).startswith('â«·dfâ«¸'):

                if not isinstance(entry[col], list):
                    raise ValueError('{} :: not a list; did you remember to do a synonym split?'.format(entry[col]))
                else:
                    result1 = []

                    for syn in entry[col]:
                        pretags = ''
                        othertags = ''

                        rules = EXDFPREP_RULES[lang]

                        if lang != 'general':
                            rules.update(EXDFPREP_RULES['general'])

                        for stage in rules:
                            for rule in rules[stage]:
                                if re.search(rule, syn):
                                    replacement, pretag, othertag = rules[stage][rule]
                                    if '\\' in pretag:
                                        pretag_addn = re.sub(rule, pretag, syn).strip()
                                        old = syn
                                        while old != pretag_addn:
                                            old = pretag_addn
                                            pretag_addn = re.sub(rule, pretag, pretag_addn).strip()
                                        pretag_addn = re.sub(paren_re, '', pretag_addn).strip()
                                        pretag_addn = re.sub('â«¸ +', 'â«¸', pretag_addn)
                                        pretags += pretag_addn
                                    else:
                                        pretags += pretag.strip()
                                    othertags += othertag.strip()
                                    syn = re.sub(rule, replacement, syn).strip()

                        if tocol >= 0:
                            entry[tocol] += pretags # new prepared column
                        else:
                            syn += pretags # right after source
                            
                        if othercol >= 0:
                            entry[othercol] += othertags

                        # pretag special language varieties:
                        if pretag_special_lvs:
                            # integers
                            int_m = re.match(r'^(?:â«·..â«¸)?([0-9]+)($|â«·)', syn.strip())
                            if int_m:
                                if int_m.group(1) in ['747','411','911','119','110']:
                                    print('WARNING: Did not pretag potentially special number:', int_m.group(1))
                                else:
                                    syn = re.sub(r'^(?:â«·[^â«¸]+â«¸)?', 'â«·ex:art-269â«¸', unicodedata.normalize('NFKC', syn)).strip()
                            # arabic nums
                            int_a_m = re.match(r'^(?:â«·..â«¸)?([Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©]+)($|â«·)', syn.strip())
                            if int_a_m:
                                if int_a_m.group(1) in []:
                                    print('WARNING: Did not pretag potentially special number:', int_a_m.group(1))
                                else:
                                    syn = re.sub(r'^(?:â«·[^â«¸]+â«¸)?', 'â«·ex:art-340â«¸', unicodedata.normalize('NFKC', syn)).strip()
                            # common han nums
                            int_h_m = re.match(r'^(?:â«·..â«¸)?([ã€‡é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬ä¸‡å„„äº¿å…†å…©ä¸¤]+|[ã€‡é›¶å£¹è²³è´°åƒåè‚†ä¼é™¸é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿ]+)($|â«·)', syn.strip())
                            if int_h_m:
                                if int_h_m.group(1) in ['ã€‡']:
                                    print('WARNING: Did not pretag potentially special number:', int_h_m.group(1))
                                else:
                                    syn = re.sub(r'^(?:â«·[^â«¸]+â«¸)?(.*)$', r'\1â«·ex:art-341â«¸\1', unicodedata.normalize('NFKC', syn)).strip()
                            # possible rare han num? print warning
                            int_rh_m = re.match(r'^(?:â«·..â«¸)?([ã€‡é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬ä¸‡å„„äº¿å…†å…©ä¸¤å…†äº¬å“ç§­ç©°æºæ²Ÿæ¾—æ¶§æ­£è¼‰è½½å»¿å…åŒçš•]+|[ã€‡é›¶å£¹è²³è´°åƒåè‚†ä¼é™¸é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿå…†äº¬å“ç§­ç©°æºæ²Ÿæ¾—æ¶§æ­£è¼‰è½½å»¿å…åŒçš•]+)($|â«·)', syn.strip())
                            if int_rh_m and any(h in x for h in 'å…†äº¬å“ç§­ç©°æºæ²Ÿæ¾—æ¶§æ­£è¼‰è½½å»¿å…åŒçš•'):
                                print('WARNING: Possible rare Han numeral, needs manual tag:', int_rh_m.group(1))
                        
                        result1.append(syn)

                    entry[col] = result1

        result.append(entry)

    return result


def mnsplit(entries, col, delim='â‹'):
    try: assert isinstance(col, int)
    except: raise ValueException('col must be integer')
    # if delim != 'â‹':
    #     entries = split_outside_parens(entries, [col], delim)
    #     entries = joincol(entries, col, 'â‹')
    result = []
    for entry in entries:
        # assert isinstance(entry[col], str)
        for mn in re.split(r'(?<!\([^\)]*)(?<!\[[^\]]*)'+delim, entry[col]):
            result.append(entry[:col] + [mn] + entry[col+1:])
    return result


def separate_parentheticals(entries, cols, delim=r'[&,;]', parens=PARENS):
    ''' Separates parentheticals within an entry, to multiple sets of parentheses (for division into
            different columns, etc)
    entries = list of entries
    cols        = list of columns (element indices) on which to perform operation
    delim     = regex of delimiters at which to split
    parens    = list of tuples of opening and closing characters (regex escaped)
                        to be considered parenthetical '''

    result = []

    for entry in entries:

        for col in cols:
            try: entry[col]
            except: raise ValueError('index', col, 'not in entry:', entry)
            for p in parens:
                o, c = p
                from_re = o + r'([^' + c + r']*)\s*' + delim + r'\s*([^' + c + r']*)' + c
                oraw, craw = o.replace('\\',''), c.replace('\\','')
                # separate parentheticals
                new = re.sub(from_re, oraw+r'\1'+craw+' '+oraw+r'\2'+craw, entry[col])
                while entry[col] != new:
                    entry[col] = new
                    new = re.sub(from_re, oraw+r'\1'+craw+' '+oraw+r'\2'+craw, new)
                entry[col] = re.sub(r'\s+\)', ')', entry[col])

        result.append(entry)

    return result


def convert_between_cols(entries, conversion_rules, fromcol, tocol=-1, syn_delim='â€£', delim='â€£'):
    ''' Convert between columns, using a dictionary/dictionaries of conversion rules.
    entries        = list of elements (columns)
    conversion_rules = dict of conversions    { from : to } OR list of dicts of conversions to do in order
    fromcol = col from which to look
    tocol     = new col in which to deposit replacements (end); if -1, use end of source col
    syn_delim = delimiter between synonyms for each entry
    delim     = delimiter to use between replacements if not a fully-formed tag '''

    assert isinstance(tocol, int)
    cols_total = len(entries[0])
    if tocol >= 0: # update source col index if making new col
        cols_total += 1
        if fromcol >= tocol: fromcol += 1

    if not isinstance(conversion_rules, list):
        conversion_rules = [conversion_rules]

    new_entries = entries
    for curr_conv_rules in conversion_rules:
        result = []
        for entry in new_entries:
            if tocol >= 0:
                if len(entry) < cols_total: # prepare new column
                    entry.insert(tocol, '')
                curr_tocol = tocol
            else:
                curr_tocol = fromcol

            colbuffer = ''

            newcol = []
            for exp in entry[fromcol].split(syn_delim):
                for rule in curr_conv_rules:
                    m = re.search(rule, exp)
                    if m:
                        # if the conversion to isn't explicit tags, add the standard delimiter
                        if 'â«·' not in curr_conv_rules[rule] and colbuffer:
                            colbuffer += delim
                        # if capture group exists, sub from the column
                        if '\\' in curr_conv_rules[rule]:
                            colbuffer += re.sub(rule, curr_conv_rules[rule], m.group(0))
                        # else just add the target
                        else:
                            colbuffer += curr_conv_rules[rule]
                        exp = re.sub(rule, ' ', exp).strip()
                        exp = re.sub(r'  +', ' ', exp.strip())
                newcol.append(exp)
            colbuffer = re.sub(r'  +', ' ', colbuffer.strip())

            entry[fromcol] = syn_delim.join(newcol)
            entry[curr_tocol] += colbuffer
            result.append(entry)
        new_entries = result

    return entries


def splitcol(entries, col, delim=''):
    result = []
    for entry in entries:
        assert isinstance(entry[col], str)
        entry[col] = [e.strip() for e in re.split(delim, entry[col]) if e.strip()]
        result.append(entry)
    return result

def joincol(entries, col, delim=''):
    result = []
    for entry in entries:
        assert isinstance(entry[col], list)
        entry[col] = delim.join(entry[col])
        result.append(entry)
    return result


def remove_nested_parens(entries, cols, parens=PARENS):
    result = []

    o_parens = [p[0] for p in parens]
    c_parens = [p[1] for p in parens]

    for entry in entries:

        for col in cols:
            try: entry[col]
            except: raise ValueError('index', col, 'not in entry:', entry)

            count = 0

            entry_letters = []

            for l in entry[col]:
                if list(filter(None, [re.match(o_paren, l) for o_paren in o_parens])):
                    if not count > 0:
                        entry_letters.append(l)
                    count += 1
                elif list(filter(None, [re.match(c_paren, l) for c_paren in c_parens])):
                    if not count > 1:
                        entry_letters.append(l)
                    count -= 1
                else:
                    entry_letters.append(l)

            entry[col] = ''.join(entry_letters)

            """
            for paren in parens:
                out_o, out_c = paren
                for paren2 in parens:
                    in_o, in_c = paren2
                    new = re.sub(r'(' + out_o + r'[^' + in_o + r']*)' + in_o + r'([^' + in_c + r']*)' + in_c + r'([^' + out_c + r']*' + out_c + r')', r'\1\2\3', entry[col]).strip()
                    while entry[col] != new:
                        entry[col] = new
                        new = re.sub(r'(' + out_o + r'[^' + in_o + r']*)' + in_o + r'([^' + in_c + r']*)' + in_c + r'([^' + out_c + r']*' + out_c + r')', r'\1\2\3', new)
            """

        result.append(entry)

    return result


def regexsubcol(refrom, reto, cols, entries):
    result = []
    for entry in entries:
        for col in cols:
            try: entry[col]
            except: raise ValueError('index', col, 'not in entry:', entry)
            entry[col] = re.sub(refrom, reto, entry[col]).strip()
        result.append(entry)
    return result


def prepsyns(entries, cols, refrom, lng, delim='â€£', pretag_special_lvs=True, othercol=-1, max_wd_single_ex=12):
    """ Splits at given regex (outside parens), runs exdfprep, joins with syn delimiter,
    and removes nested parens.
    entries = list of entries
    cols        = cols on which to operate
    refrom    = regex to split synonyms
    lng         = language, for exdfprep
    delim     = output delimiter for synonyms
    """
    assert isinstance(cols, list)
    result = []
    # split at given delimiter
    entries = split_outside_parens(entries, cols, refrom, max_wd_single_ex=max_wd_single_ex)
    # prepare as expression
    entries = exdfprep(entries, cols, lang=lng, pretag_special_lvs=pretag_special_lvs, othercol=othercol)
    # join with consistent synonym delimiter
    for entry in entries:
        for col in cols:
            entry[col] = delim.join(nodupes(filter(None, entry[col])))
        result.append(entry)
    # remove nested parens
    result = remove_nested_parens(entries, cols)
    # detect taxa, report if extract_taxa is advised
    # detect_taxa(entries, cols, delim=delim)
    return result

def nodupes(ls):
    result = []
    for l in ls:
        if l not in result:
            result.append(l)
    return result

def resolve_xrefs(entries, fromcol, hwcol, xref_format=r'^= (.+)$', delim='â€£'):
    """ Resolve cross-references by copying information from other entries.
    entries = list of entries
    fromcol = column with cross-references to resolve
    hwcol     = column with headwords that are being referenced
    xref_format = regex to detect crossrefs. capture group 1 should match a potential headword.
    """
    xref_marker = 'ğ€¿'
    result = []
    for entry in entries:
        try: entry[fromcol]
        except: raise ValueError('index', fromcol, 'not in entry:', entry)
        m = re.match(xref_format, entry[fromcol])
        if m:
            try: hw = entry[hwcol]
            except: raise ValueError('index', hwcol, 'not in entry:', entry)
            try: xref = m.group(1)
            except: raise ValueError('crossref format regex has no capture group:', xref_format)
            for entry2 in entries:
                try: entry2[hwcol]
                except: raise ValueError('index', hwcol, 'not in entry:', entry2)
                if entry2[hwcol] == xref:
                    entry2 = entry2[:fromcol] + [xref_marker+entry2[fromcol]] + entry2[fromcol+1:]
                    entry2 = entry2[:hwcol] + [hw] + entry2[hwcol+1:]
                    result.append(entry2)
        else: # no match, just pass through
            result.append(entry)

    # step2: if fromcols match, group the two together as a synonym pair
    result2 = []
    done_indices = []
    for i, entry in enumerate(result):
        if i not in done_indices:
            hws, to = [entry[hwcol]], entry[fromcol]
            for j, entry2 in enumerate(result):
                hw2, to2 = entry2[hwcol], entry2[fromcol]
                if xref_marker+to == to2:
                    hws.append(hw2)
                    done_indices.append(j)
            result2.append(entry[:hwcol] + [delim.join(hws)] + entry[hwcol+1:])

    result2 = [entry for entry in result2 if not entry[fromcol].startswith(xref_marker)]

    return result2


def get_normalize_scores(exps, lang='eng-000'):
    """
    exps = list of expressions to get scores
    lang = panlex language variety, including hyphen and 3 digit id
    returns dict of expressions and their scores
    """
    assert isinstance(exps, list)

    try:
        urllib.request
    except:
        import urllib.request

    try:
        sleep
    except:
        from time import sleep

    try:
        json
    except:
        import simplejson as json

    url = 'http://api.panlex.org/norm/ex/' + lang

    # build proper query string
    query  = '{ "tt" : ["'
    query += '","'.join([exp.replace('"', '\\"') for exp in exps])
    query += '"] }'

    req = urllib.request.Request(url, query.encode('utf-8'))

    with urllib.request.urlopen(req) as r:
        response = r.read().strip()

    result = json.loads(response)

    scores = {}
    for exp in exps:
        scores[exp] = result['norm'][exp]['score']

    sleep(0.5)

    return scores


def normalize(entries, col, threshold=50, lang='eng-000'):
    """ keeps/discards entries based on expressions in given column. """
    assert col < len(entries[0])
    result = []
    scores = get_normalize_scores([entry[col] for entry in entries])
    for entry in entries:
        if scores[entry[col]] >= threshold:
            result.append(entry)
    return result


JPN_NORMALIZE_EXCEPTIONS = ['ãŠã­ãˆã¡ã‚ƒã‚“', 'ãŠã«ã„ã¡ã‚ƒã‚“', 'ã®ã‚“ã¹ãˆ', 'ä¸€äººä¸€äºº', 'è³ªã®ã‚ˆããªã„']

def jpn_normalize(entries, col, delim='â€£', maxlen=3, dftag='â«·dfâ«¸', suppress_print=True):
    """ Keeps, or retags entries based on expressions in given column. """
    try:
        MecabSoup
    except:
        from MecabSoup import MecabSoup

    paren_regex = DEFAULT_PR

    assert col < len(entries[0])
    result = []
    for entry in entries:
        if not isinstance(entry[col], list):
            entry[col] = entry[col].split(delim)
        newentry = []
        for syn in entry[col]:
            syn_noparens = re.sub(paren_regex, '', syn).strip()
            syn_noparens = re.sub(r'â«·.*', '', syn_noparens).strip()
            syn_noparens = re.sub(r'\s*â€¦\s*', '', syn_noparens).strip()
            for exc in JPN_NORMALIZE_EXCEPTIONS:
                syn_noparens = re.sub(exc, 'X', syn_noparens).strip()
            if len(syn_noparens) <= maxlen or not re.sub(r'\p{Katakana}+','',syn_noparens).strip(): # too short to analyze, or all katakana
                newentry.append(syn)
                if not suppress_print:
                    print(syn)
            else:
                if MecabSoup(syn_noparens).length > maxlen or 'ã€‚' in syn_noparens:
                    # retag
                    newentry.append(dftag+syn)
                    if not suppress_print:
                        print(dftag+syn)
                else:
                    newentry.append(syn)
                    if not suppress_print:
                        print(syn)
        result.append(entry[:col] + [delim.join(newentry).replace(delim+dftag[0],dftag[0])] + entry[col+1:])
    return result


def lemmatize_verb(text):
    try:
        TextBlob
    except:
        from textblob import TextBlob

    try:
        nltk
    except:
        import nltk

    ops = r'|'.join([p[0] for p in PARENS])
    cps = r'|'.join([p[1] for p in PARENS])

    text_annot = []
    paren_counter = 0
    last_paren_word = False
    for word in text.split(' '):
        for letter in word:
            if re.match(ops, letter):
                paren_counter += 1
            elif re.match(cps, letter):
                paren_counter -= 1
                if paren_counter == 0:
                    last_paren_word = True
        text_annot.append((word, paren_counter == 0 and not last_paren_word))
        last_paren_word = False


    to_blob = ' '.join([w[0] for w in text_annot if w[1]])
    to_blob = re.sub(r'\s+,', ',', to_blob).strip()
    to_blob = re.sub(r' n\'t', 'n\'t', to_blob).strip()
    to_blob = to_blob.split(' ')

    blob = TextBlob(' '.join(to_blob))

    assert(len(blob.words) == len(to_blob))

    if ('it' in text and not text.startswith('it')) or 'him' in text:
        tags = nltk.pos_tag(['it'] + blob.words)[1:]
    else:
        tags = nltk.pos_tag(blob.words)


    lemmatized_verbs = ' '.join([TextBlob(tags[i][0]).words[0].lemmatize('v') if tags[i][1] in ['VB','VBD','VBP','VBZ'] else to_blob[i] for i in range(len(tags))])

    print(blob.words, '-->', tags, '---->', lemmatized_verbs)

    return lemmatized_verbs



def single_expsplit(exp, splitdelim='/', expdelim='â€£'):
    """ splits individual expressions based on some punctuation cue """
    assert isinstance(exp, str)
    p = DEFAULT_PR[1:-1]
    prs = ''.join([r[0] for r in PARENS]) + ''.join([r[1] for r in PARENS])
    done = False
    while not done:
        exp = exp.split(expdelim)
        newexp = []
        for ex in exp:
            newex = ex
            if splitdelim == '/': # fwd slash, no space
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^/\s'+prs+']+)/([^\s'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+r')*)$', r'\1\2\3\5\6'+expdelim+r'\1\2\4\5\6', newex)

            elif splitdelim == '()': # parens, space agnostic
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']+)\(([^\)\s]+)\)([^'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']*)\(([^\)\s]+)\)([^'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)

            elif splitdelim == '(ns)': # parens, no space
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']+)\(([^\)\s]+)\)([^\s'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']*)\(([^\)\s]+)\)([^\s'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)

            elif splitdelim == '[]': # brackets, space agnostic
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']+)\[([^\]]+)\]([^'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']*)\[([^\]]+)\]([^'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)

            elif splitdelim == '[ns]': # brackets, no space
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']+)\[([^\]]+)\]([^\s'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']*)\[([^\]]+)\]([^\s'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)

            elif splitdelim == '{}': # braces, space agnostic
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']+)\{([^\}]+)\}([^'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^'+prs+']*)\{([^\}]+)\}([^'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)

            elif splitdelim == '{ns}': # braces, no space
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']+)\{([^\}]+)\}([^\s'+prs+']*)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
                newex = re.sub(r'^((?:[^'+prs+']|'+p+')*)(^|\s)([^\s'+prs+']*)\{([^\}]+)\}([^\s'+prs+']+)(\s|$)((?:[^'+prs+']|'+p+')*)$', r'\1\2\3\5\6\7'+expdelim+r'\1\2\3\4\5\6\7', newex)
            else:
                raise ValueException('invalid delimiter')
            newex = re.sub(r'\s*'+expdelim+r'\s*', expdelim, newex).strip()
            newex = re.sub(r'\s\s+', ' ', newex).strip()
            newexp.append(newex)
        if set(newexp) == set(exp):
            done = True
        exp = expdelim.join(newexp)
    return exp


def expsplit(entries, cols, splitdelim='/', expdelim='â€£'):
    for col in cols:
        assert col < len(entries[0])
    result = []
    for entry in entries:
        newentry = []
        for col in range(len(entry)):
            newcol = entry[col]
            if col in cols:
                splitcol = newcol if isinstance(newcol, list) else newcol.split(expdelim)
                newcol = [single_expsplit(exp, splitdelim, expdelim) for exp in splitcol]
                newcol = expdelim.join(newcol)
            newentry.append(newcol)
        result.append(newentry)
    return result

NO_DECAP = ["Africa","AIDS","Albania","America","American","Antarctica","April","Argentina","Asia","August","Australia","Austria","Belgium","Brazil","Buddhist","Bulgaria","Canada","Chanukah","China","Christmas","Colombia","Cuba","Czech Republic","December","Denmark","Easter","Egypt","English","Europe","European","February","Finland","France","French","Friday","Germany","Great Britain","Greece","Hindu","Hungary","I","I.D.","ID","India","Indian","Indonesia","Internet","Iran","Iraq","Ireland","Israel","Italian","Italy","January","Japan","Jewish","July","June","Jupiter","Mandarin","March","Mars","May","Mercury","Mexico","Monday","Mongolia","Ms.","Mr.","Neptune","Netherlands","North America","North Korea","Norway","November","October","Pakistan","Pluto","Poland","Pope","Portugal","Portuguese","Romania","Romanian","Romanova","Russia","Saturday","Saturn","Saudi Arabia","September","Slovak Republic","South Africa","South America","South Korea","Spain","Spanish","Sunday","Sweden","Switzerland","Thailand","Thursday","Tuesday","God","Turkey","Ukraine","United Kingdom","United States","Uranus","Venezuela","Venus","Vietnam","Wednesday","Yugoslavia","Ashavan","Asha","Zarathushtra","Ahura","Khwarrah","Fravashi","Fravashis","Mazda","Mithra"]

def decap(entries, cols):
    # decapitalize the first (after parens) letter of each given column (except for some key words)
    for col in cols:
        assert col < len(entries[0])
    result = []
    for entry in entries:
        newentry = []
        for col in range(len(entry)):
            newcol = entry[col]
            if col in cols:
                m = re.match(r'^('+DEFAULT_PR_NOCAP+r'?)\s*(.*)$', newcol)
                if not m:
                    raise ValueError('unexpected non-match:', newcol)
                else:
                    paren, rest = m.group(1), m.group(2)
                    if not list(filter(None, [re.search(r'^'+nd+r'(?:\s|â€£|â«·|;|,|\.|:|$)', rest) for nd in NO_DECAP])):
                        newcol = paren + ' ' + rest[0].lower() + rest[1:] if rest else ''
                        newcol = newcol.strip()
            newentry.append(newcol)
        result.append(newentry)
    return result


def tsv_to_entries(infile):
    entries = [line.split('\t') for line in infile.readlines()]
    return preprocess(entries)

def delete_col(entries, col):
    assert col < len(entries[0])
    return [entry[:col] + entry[col+1:] for entry in entries]

def delete_cols(entries, cols):
    result = entries
    for col in sorted(cols, reverse=True):
        result = delete_col(result, col)
    return result

def correct_homoglyphs(entries, cols, target_script='Latn'):
    # convert any stray characters from other scripts (Latin, Cyrillic, Greek)
    # to their homoglyphs in the target_script script
    for col in cols:
        assert col < len(entries[0])
    assert target_script in ['Latn', 'Cyrl', 'Grek']
    if not HOMOGLYPH_DICTS: __init_homoglyph_dicts()
    result = []
    for entry in entries:
        newentry = []
        for col in range(len(entry)):
            newcol = entry[col]
            if col in cols:
                newcol = ''.join([HOMOGLYPH_DICTS[target_script][c] if c in HOMOGLYPH_DICTS[target_script].keys() else c for c in newcol])
            newentry.append(newcol)
        result.append(newentry)
    return result

HOMOGLYPH_DICTS = {}

def __init_homoglyph_dicts():
    HOMOGLYPHS = [
        ('A','Î‘','Ğ'),
        ('B','Î’','Ğ’'),
        ('C','Ï¹','Ğ¡'),
        ('E','Î•','Ğ•'),
        ('F','Ïœ',''),
        ('G','','ÔŒ'),
        ('H','Î—','Ğ'),
        ('I','Î™','Ğ†'),
        ('J','','Ğˆ'),
        ('K','Îš','Ğš'),
        ('M','Îœ','Ğœ'),
        ('N','Î',''),
        ('O','ÎŸ','Ğ'),
        ('P','Î¡','Ğ '),
        ('S','','Ğ…'),
        ('T','Î¤','Ğ¢'),
        ('V','','Ñ´'),
        ('X','Î§','Ğ¥'),
        ('Y','Î¥','Ò®'),
        ('Z','Î–',''),
        ('a','Î±','Ğ°'),
        ('b','Î²','Ğ¬'),
        ('c','Ï²','Ñ'),
        ('d','','Ô'),
        ('e','Îµ','Ğµ'),
        ('h','','Ò»'),
        ('i','Î¹','Ñ–'),
        ('j','','Ñ˜'),
        ('k','Îº','Ğº'),
        ('o','Î¿','Ğ¾'),
        ('Å‹','Î·',''),
        ('p','Ï','Ñ€'),
        ('s','Ï‚','Ñ•'),
        ('t','Ï„','Ñ‚'),
        ('v','Î½','Ñµ'),
        ('w','','Ñ¡'),
        ('x','Ï‡','Ñ…'),
        ('y','Î³','Ñƒ'),
        ('Ã„','','Ó’'),
        ('Ã‹','','Ğ'),
        ('Ã–','','Ó¦'),
        ('Ã¤','','Ó“'),
        ('Ã«','','Ñ‘'),
        ('Ã¶','','Ó§'),
        ('É™','','Ó™'),
        ('Ã','Ï·',''),
        ('Ã¾','Ï¸',''),
        ('Ã†','Ó”',''),
        ('Ã¦','Ó•',''),
        #('ÃŸ','Î²',''),
    ]
    TO_LATN, TO_CYRL, TO_GREK = {}, {}, {}
    for triple in HOMOGLYPHS:
        latn, grek, cyrl = triple
        if latn and grek:
            TO_LATN[grek] = latn
            TO_GREK[latn] = grek
        if latn and cyrl:
            TO_LATN[cyrl] = latn
            TO_CYRL[latn] = cyrl
        if grek and cyrl:
            TO_GREK[cyrl] = grek
            TO_CYRL[grek] = cyrl
    global HOMOGLYPH_DICTS
    HOMOGLYPH_DICTS = {
        'Latn' : TO_LATN,
        'Cyrl' : TO_CYRL,
        'Grek' : TO_GREK
    }


def __all_vowels_to_a(s, vowels='AEIOUYaeiouy'):
    return ''.join(['a' if unidecode(letter) in vowels else letter for letter in s])

def __degrade(s):
    return s.replace('j','i').replace('w','u')

def synthesize_strings(s1, s2, max_overlap=4, vowels='AEIOUYaeiouy'):
    # return a synthesis of the first and second strings, based on max_overlap amount
    max_overlap = min(max_overlap, min(len(s1),len(s2))) # account for string lengths

    # first check for exact matches, then inexact matches, on all lengths except 1
    for overlap in range(max_overlap, 1, -1):
        for match in range(overlap, 1, -1):
            s1p = s1[-overlap:] if overlap == match else s1[-overlap:-overlap+match]
            s2p = s2[:match]
            s1pd = __all_vowels_to_a(__degrade(s1p))
            s2pd = __all_vowels_to_a(__degrade(s2p))
            if s1p == s2p or s1pd == s2pd:
                return s1[:-overlap]+s2
    # now length 1 exact, or degraded
    for overlap in range(max_overlap, 0, -1):
        s1l = s1[-overlap]
        s2l = s2[0]
        s1ld = __degrade(s1l)
        s2ld = __degrade(s2l)
        if s1l == s2l or s1ld == s2ld:
            return s1[:-overlap]+s2

    return s1+s2

def insert_into_tilde(s1, s2, max_overlap=4, vowels='AEIOUYaeiouy'):
    # choose how best to synthesize strings depending on context
    # first make sure a single tilde is in s2 in the first place
    s2_separated = re.match(r'^([^~]*)~([^~]*)$', s2)
    if not s2_separated:
        # print('tilde count != 1, returning string as is:', s2)
        return s2

    s2_pre, s2_post = s2_separated.group(1), s2_separated.group(2)

    if s2_pre.endswith(' ') and s2_post.startswith(' '):
        # tilde surrounded by space, just put s1 in there
        return s2.replace('~', s1)
    elif s2_pre.endswith(' ') or s2_post.startswith('~'):
        # tilde at beginning/after space
        return s2_pre + synthesize_strings(s1, s2_post)
    elif s2_pre.endswith('~') or s2_post.startswith(' '):
        # tilde at end/before space
        return synthesize_strings(s2_pre, s1) + s2_post
    else:
        # ambiguous, so do two merges
        return synthesize_strings(synthesize_strings(s2_pre, s1), s2_post)


def extract_taxa(entries, col, delim='â€£'):
    # reqno = 0
    print('\nextracting taxa...')
    try:
        requests
    except:
        import requests
    try:
        # reqno += 1
        r = requests.get('http://127.0.0.1:3000', params={'text': 'string'})
    except:
        raise ConnectionError('Must initialize taxonfinder')
    else:
        result = []
        for entry in entries:
            newentry = []
            for syn in entry[col].split(delim):
                if re.search(r'\p{Latin}', syn.replace('â«·dfâ«¸','')):
                    r = requests.get('http://127.0.0.1:3000', params={'text': syn}, headers={'Connection':'close'}).json()
                    if r:
                        for match in r:
                            offsets, name = match['offsets'], match['name']
                            print(name)
                            if offsets[1] - offsets[0] == len(syn):
                                syn = 'â«·ex:lat-003â«¸' + name
                            else:
                                # detect "sp(p)." notation and add meaning classification
                                if re.search(name + ' spp?\.?', syn):
                                    syn = re.sub(name + ' spp?\.?', ' ', syn).strip()
                                    syn += 'â«·mcs2:art-300â«¸IsAâ«·mcs:lat-003â«¸' + name
                                if syn.startswith('â«·mcs'):
                                    syn = 'â«·ex:lat-003â«¸' + name + syn
                                else:
                                    syn += 'â«·ex:lat-003â«¸' + name
                newentry.append(syn)
            entry[col] = delim.join(newentry)

        return entries
        print('done')


def get_next_sibling_tag(tag):
    # next tag method for beautifulsoup (siblings only)
    for nextsib in tag.next_siblings:
        if isinstance(nextsib, Tag):
            return nextsib
    return None

def get_prev_sibling_tag(tag):
    # previous tag method for beautifulsoup (siblings only)
    for prevsib in tag.previous_siblings:
        if isinstance(prevsib, Tag):
            return prevsib
    return None

def get_next_tag(tag):
    # next tag method for beautifulsoup
    for nexttag in tag.next_elements:
        if isinstance(nexttag, Tag):
            return nexttag
    return None
    
def get_prev_tag(tag):
    # previous tag method for beautifulsoup
    for prevtag in tag.previous_elements:
        if isinstance(prevtag, Tag):
            return prevtag
    return None

def classify_cmn(entries, col, delim='â€£'):
    """ Classifies unknown cmn as simplified/traditional/both. """
    try:
        hantool
    except:
        from hantool import HanText

    ht = HanText('')

    simptag, tradtag = 'â«·ex:cmn-000â«¸', 'â«·ex:cmn-001â«¸'

    assert col < len(entries[0])
    result = []
    for entry in entries:
        if not isinstance(entry[col], list):
            entry[col] = entry[col].split(delim)
        newentry = []
        for syn in entry[col]:
            if syn:
                ht.update(syn)
                if ht.is_simp():
                    # print(simptag+syn)
                    newentry.append(simptag+syn)
                if ht.is_trad():
                    # print(tradtag+syn)
                    newentry.append(tradtag+syn)
                # if ht.is_trad_only():
                #     print('!!!', syn)
        result.append(entry[:col] + [delim.join(newentry).replace(delim+simptag[0],simptag[0])] + entry[col+1:])
    return result


def tb_to_wn(tb):
    # Convert Penn Treebank to WordNet POS tags
    tb = tb[:2] if tb[:2] in ['NN','JJ','VB'] else 'RB'
    return {'NN':wn.NOUN,'JJ':wn.ADJ,'VB':wn.VERB,'RB':wn.ADV}[tb]
